{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lordpatil/.pyenv/versions/3.12.2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "from moviepy.editor import VideoFileClip, ImageClip, CompositeVideoClip, AudioFileClip, concatenate_videoclips\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell relies on the 'client' variable (Gemini API client)\n",
    "# being initialized in a subsequent cell (e.g., cell 5) and 'genai'\n",
    "# (google.generativeai module) being imported (e.g., cell 1).\n",
    "# Please ensure the cell initializing 'client' is executed before this one\n",
    "# if you are running cells out of order or individually.\n",
    "\n",
    "# 1. Define the path to your image and your question.\n",
    "# TODO: Replace \"your_image.png\" with the actual path to your image file.\n",
    "# You might need to create a dummy \"your_image.png\" in the same directory\n",
    "# as this notebook or use an existing image file.\n",
    "image_file_path = \"your_image.png\"  # Ensure this file exists or is created by you\n",
    "question_text = \"What do you see in this image? Describe it in detail.\"\n",
    "\n",
    "# Check if 'client' and 'genai' are available, as they are initialized/imported elsewhere.\n",
    "# Also check if 'os' module (for os.path.exists) is available from cell 1.\n",
    "if 'client' not in globals() or client is None:\n",
    "    print(\"Error: The 'client' object (Gemini API client) is not yet initialized.\")\n",
    "    print(\"Please run the cell that initializes 'client' (likely cell 5) and then re-run this cell.\")\n",
    "elif 'genai' not in globals():\n",
    "    print(\"Error: The 'genai' module (google.generativeai) is not imported.\")\n",
    "    print(\"Please run the cell that imports 'genai' (likely cell 1 or 4) and then re-run this cell.\")\n",
    "elif 'os' not in globals():\n",
    "    print(\"Error: The 'os' module is not imported.\")\n",
    "    print(\"Please ensure 'import os' is present and executed (likely in cell 1).\")\n",
    "elif not os.path.exists(image_file_path):\n",
    "    print(f\"Error: Image file not found at '{image_file_path}'.\")\n",
    "    print(f\"Please create the file '{image_file_path}' or update the path variable in this cell.\")\n",
    "else:\n",
    "    try:\n",
    "        # 2. Upload the image file to Gemini.\n",
    "        print(f\"Uploading '{image_file_path}' to Gemini...\")\n",
    "        # 'client' should be an initialized genai.Client instance.\n",
    "        # 'genai' should be the imported google.generativeai module.\n",
    "        uploaded_file = client.upload_file(path=image_file_path)\n",
    "        print(f\"Successfully uploaded file: {uploaded_file.name} (URI: {uploaded_file.uri})\")\n",
    "\n",
    "        # 3. Select a multimodal model.\n",
    "        # \"gemini-1.5-flash-latest\" is a fast and capable model for image understanding.\n",
    "        # Other options include \"gemini-pro-vision\".\n",
    "        model = genai.GenerativeModel(model_name=\"gemini-1.5-flash-latest\")\n",
    "\n",
    "        # 4. Generate content (ask the question about the image).\n",
    "        print(f\"\\nAsking Gemini: '{question_text}'\")\n",
    "        # The API expects a list of parts, which can be the uploaded file and text.\n",
    "        response = model.generate_content([uploaded_file, question_text])\n",
    "\n",
    "        # 5. Print Gemini's response.\n",
    "        print(\"\\nGemini's Response:\")\n",
    "        if response.text:\n",
    "            print(response.text)\n",
    "        else:\n",
    "            print(\"Received no text in response. This might happen if the prompt was blocked or an issue occurred.\")\n",
    "            # Check for prompt feedback, which can indicate safety blocks.\n",
    "            if hasattr(response, 'prompt_feedback') and response.prompt_feedback:\n",
    "                print(f\"Prompt Feedback: {response.prompt_feedback}\")\n",
    "            else:\n",
    "                # Print the full response object for more detailed inspection if text is missing.\n",
    "                print(f\"Full response object for debugging: {response}\")\n",
    "        \n",
    "        # Optional: If you want to delete the file from Gemini storage after use to manage storage:\n",
    "        # print(f\"\\nDeleting uploaded file '{uploaded_file.name}' from Gemini storage...\")\n",
    "        # client.delete_file(name=uploaded_file.name)\n",
    "        # print(\"File deleted successfully from Gemini storage.\")\n",
    "\n",
    "    except AttributeError as e:\n",
    "        # This can happen if 'client' or 'genai' is None or not the expected type.\n",
    "        print(f\"An AttributeError occurred: {e}\")\n",
    "        print(\"This might indicate that 'client' or 'genai' was not properly initialized or has become None.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while interacting with the Gemini API: {e}\")\n",
    "        # For more detailed debugging information, you can uncomment the following lines:\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-genai>1.8.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m client = \u001b[43mgenai\u001b[49m.Client(api_key=os.getenv(\u001b[33m'\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      2\u001b[39m VEO_MODEL_ID = \u001b[33m\"\u001b[39m\u001b[33mveo-2.0-generate-001\u001b[39m\u001b[33m\"\u001b[39m \n",
      "\u001b[31mNameError\u001b[39m: name 'genai' is not defined"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=os.getenv('GOOGLE_API_KEY'))\n",
    "VEO_MODEL_ID = \"veo-2.0-generate-001\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_am_aware_that_veo_is_a_paid_feature_and_am_OK_with_paying_to_run_this_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from google.genai import types\n",
    "from IPython.display import Video, HTML\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='models/veo-2.0-generate-001/operations/k9hjd1hn3qzc' metadata=None done=None error=None response=None result=None\n",
      "name='models/veo-2.0-generate-001/operations/k9hjd1hn3qzc' metadata=None done=True error=None response=GenerateVideosResponse(generated_videos=None, rai_media_filtered_count=1, rai_media_filtered_reasons=[\"1 videos were filtered out because they violated Google's Responsible AI practices. You will not be charged for blocked videos. Try rephrasing the prompt. If you think this was an error, send feedback.\"]) result=GenerateVideosResponse(generated_videos=None, rai_media_filtered_count=1, rai_media_filtered_reasons=[\"1 videos were filtered out because they violated Google's Responsible AI practices. You will not be charged for blocked videos. Try rephrasing the prompt. If you think this was an error, send feedback.\"])\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(operation)\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(operation.result.generated_videos)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n, generated_video \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerated_videos\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     67\u001b[39m   client.files.download(file=generated_video.video)\n\u001b[32m     68\u001b[39m   generated_video.video.save(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mvideo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mp4\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Saves the video(s)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "if not I_am_aware_that_veo_is_a_paid_feature_and_am_OK_with_paying_to_run_this_colab:\n",
    "  print(\"Veo is a paid feature. Please change the variable 'I_am_aware_that_veo_is_a_paid_feature_and_am_OK_with_paying_to_run_this_colab' to True if you are okay with paying to run it.\")\n",
    "\n",
    "else:\n",
    "  import time\n",
    "  from google.genai import types\n",
    "  from IPython.display import Video, HTML\n",
    "\n",
    "  prompt = \"\"\"\n",
    "\"description\": \"Continues from Scene 1. The defeated person is still slumped on the beanbag. A pair of pristine white Blue Dragon crew socks — matching the style, color, and branding of the reference image — are almost magically presented, perhaps floating into frame or dramatically pulled out from a nearby cushion. The reference image should influence only the sock design and texture details, not the scene layout or composition. A close-up shows the person's expression shift from despair to hopeful intrigue as they grasp the socks. They quickly pull them on, with a satisfying 'whoosh' sound effect. As the second sock settles, their entire demeanor transforms. Their face lights up with pure bliss. They spring up from the beanbag with impossible lightness, immediately resuming their 'comfort dance' from Scene 1, but this time, they are graceful, floating, and gliding effortlessly across the hard floor. They might even execute a comical, smooth moonwalk or an exaggeratedly soft tap-dance, ending with a final shot of their feet, perfectly cushioned in the white Blue Dragon socks, looking incredibly comfortable and bouncy.\",\n",
    "\n",
    "\"visual_cues\": [\n",
    "  \"Same person slumped on beanbag.\",\n",
    "  \"Close-up on the pristine white Blue Dragon crew socks — design influenced only by reference image, not pose or background.\",\n",
    "  \"Slow-motion shot of the person pulling on the socks, highlighting the plush interior and branding.\",\n",
    "  \"Instantaneous facial transformation from agony to euphoria.\",\n",
    "  \"Person springs up, performing a light, 'floating' version of their previous dance, exaggeratedly smooth movements.\",\n",
    "  \"Final shot focused on the feet, showcasing the white Blue Dragon socks looking supremely comfortable on the hard floor.\"\n",
    "],\n",
    "\n",
    "\"style_reference_note\": \"The image is to be used strictly for sock design reference — for color, material, and branding placement — and should not influence framing, lighting, pose, or environment.\"\n",
    "\"\"\" # @param {type: \"string\"}\n",
    "\n",
    "  # Here are a few prompts to help you get started and spark your creativity:\n",
    "  # 1. Wide shot of a futuristic cityscape at dawn. Flying vehicles zip between skyscrapers. Camera pans across the skyline as the sun rises.\n",
    "  # 2. A close up of a thief's gloved hand that reaches for a priceless diamond necklace in a museum display case. Camera slowly tracks the hand, with dramatic lighting and shadows.\n",
    "  # 3. A giant, friendly robot strolls through a field of wildflowers, butterflies fluttering around its head. Camera tilts upwards as the robot looks towards the sky.\n",
    "  # 4. A single, perfectly ripe apple hangs from a branch. It is covered in dew. A gentle breeze sways the branch, causing the apple to rotate slowly.\n",
    "  # 5. A beehive nestled in a hollow tree trunk in a magical forest. Bees fly in and out of the hive, carrying pollen and nectar\n",
    "  # 6. In a beautiful field of flowers, show a cute bunny that is slowly revealed to be an eldritch horror from outside time and space.\n",
    "\n",
    "  # Optional parameters\n",
    "  negative_prompt = \"\" # @param {type: \"string\"}\n",
    "  person_generation = \"allow_adult\"  # @param [\"dont_allow\", \"allow_adult\"]\n",
    "  aspect_ratio = \"9:16\" # @param [\"16:9\", \"9:16\"]\n",
    "  number_of_videos = 1 # @param {type:\"slider\", min:1, max:4, step:1}\n",
    "  duration = 8 # @param {type:\"slider\", min:5, max:8, step:1}\n",
    "  im = Image.open('/Users/lordpatil/Projects/LordSocks Marketing Pipeline/socks_reels_pipeline/assets/product_images/oriental.jpg')\n",
    "\n",
    "  # converting the image to bytes\n",
    "  image_bytes_io = io.BytesIO()\n",
    "  im.save(image_bytes_io, format=im.format)\n",
    "  image_bytes = image_bytes_io.getvalue()\n",
    "  operation = client.models.generate_videos(\n",
    "      model=VEO_MODEL_ID,\n",
    "      prompt=prompt,\n",
    "      image=types.Image(image_bytes=image_bytes, mime_type=im.format),\n",
    "      config=types.GenerateVideosConfig(\n",
    "        # At the moment the config must not be empty\n",
    "        person_generation=person_generation,\n",
    "        aspect_ratio=aspect_ratio,  # 16:9 or 9:16\n",
    "        number_of_videos=number_of_videos, # supported value is 1-4\n",
    "        negative_prompt=negative_prompt,\n",
    "        duration_seconds=duration, # supported value is 5-8\n",
    "      ),\n",
    "  )\n",
    "\n",
    "  # Waiting for the video(s) to be generated\n",
    "  while not operation.done:\n",
    "      time.sleep(20)\n",
    "      operation = client.operations.get(operation)\n",
    "      print(operation)\n",
    "\n",
    "  print(operation.result.generated_videos)\n",
    "\n",
    "  for n, generated_video in enumerate(operation.result.generated_videos):\n",
    "    client.files.download(file=generated_video.video)\n",
    "    generated_video.video.save(f'video{n}.mp4') # Saves the video(s)\n",
    "    display(generated_video.video.show()) # Displays the video(s) in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
